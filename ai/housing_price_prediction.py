# -*- coding: utf-8 -*-
"""housing_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnYlCZjCj32U_QDyk-PHWXE_WgflLMY5

# **Imports**
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import ConfusionMatrixDisplay

import statsmodels.api as sm

"""# **For Google Colab (Only run on Colab)**"""

from google.colab import drive
drive.mount('/content/drive')

# Load timeseries dataset
time_series_df = pd.read_csv('/content/drive/MyDrive/Group1017-source_code/datasets/time_series_by_suburb.csv')

# Load the suburb to zipcode mapping dataset
zipcode_df = pd.read_csv('/content/drive/MyDrive/Group1017-source_code/datasets/suburb_to_zipcode.csv')

# Load housing dataset
house_df = pd.read_csv('/content/drive/MyDrive/Group1017-source_code/datasets/housing_features_by_address.csv')

"""# **For JupyterNotebook or Local (Only run on local)**"""

# Load timeseries dataset
time_series_df = pd.read_csv('datasets/time_series_by_suburb.csv')

# Load the suburb to zipcode mapping dataset
zipcode_df = pd.read_csv('datasets/suburb_to_zipcode.csv')

# Load housing dataset
house_df = pd.read_csv('datasets/housing_features_by_address.csv')

"""# **Data Processing**

## Data Cleaning

Such as handling missing value, dropping columns that arent needed

### ***Housing Price Time Series By Suburb***
"""

time_series_df

# Define the new column names
new_columns = ['Locality', 'drop', 'drop', 'drop', '2016', '2017', '2018', '2019',
               '2020', '2021', '2022', '2023', 'drop', '22-23', 'drop', 'drop', 'drop', 'drop']

# Rename the columns
time_series_df.columns = new_columns

# Drop the 'drop' columns
time_series_df = time_series_df.drop(columns=['drop'])

time_series_df

# Drop rows with NaN values (which might occur after coercion of non-numeric strings)
time_series_df = time_series_df.dropna(how='any')

# Reset the index after dropping rows
time_series_df = time_series_df.drop(index=0)
time_series_df = time_series_df.reset_index(drop=True)

# Convert all columns except 'Locality' to numeric (forcing errors to NaN if any non-numeric values are present)
for col in time_series_df.columns[1:]:  # Exclude the first column, because its a repeat
    time_series_df[col] = pd.to_numeric(time_series_df[col], errors='coerce')

# Drop rows with any NaN values
time_series_df.dropna(inplace=True)

time_series_df.isna().sum()

"""Feature Engineer"""

price_df = time_series_df.copy()
price_df

# Set 'Locality' as the index
price_df.set_index('Locality', inplace=True)

# Calculate year-to-year percentage changes with rounding
for year in range(2016, 2023):  # Loop through 2013 to 2022
    next_year = year + 1
    price_df[f'{year}to{next_year}'] = round(((price_df[str(next_year)] - price_df[str(year)]) / price_df[str(year)]) * 100, 0)

# Reset the index 'Locality'
price_df.reset_index(inplace=True)

# Display the updated DataFrame with new columns
print(price_df)

#Drop columns
price_df = price_df.drop(columns=['2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '22-23'])

price_df

"""Merging with Zipcode"""

zipcode_df

#Drop all rows that are not "VIC"
zipcode_df = zipcode_df[zipcode_df['State'] == 'VIC'].reset_index(drop=True)

zipcode_df

# Merge the two DataFrames based on the standardized locality
merged_df = pd.merge(price_df, zipcode_df, left_on='Locality', right_on='Suburb', how='left')

# Fill missing zip codes with a default value (like 'Unknown')
merged_df['Zip'] = merged_df['Zip'].fillna('Unknown')

merged_df

# Remove rows with "Unknown" value in the Zip column
merged_df = merged_df[merged_df['Zip'] != 'Unknown']

# Ensure Zip column is not a string and convert it to numeric if it is
merged_df['Zip'] = pd.to_numeric(merged_df['Zip'], errors='coerce')

# Remove rows with null values in the 'Zip' column
merged_df = merged_df.dropna(subset=['Zip'])

# Convert 'Postcode' column to integer type
merged_df['Zip'] = merged_df['Zip'].astype(int)

# Rename Zip to Postcode
merged_df = merged_df.rename(columns={'Zip': 'Postcode'})

# Drop unnecessary columns
merged_df = merged_df.drop(['Locality', 'State', 'Suburb'], axis=1)

# Reset the index after removing rows
merged_df = merged_df.reset_index(drop=True)

merged_df

"""### ***Housing Features by Address***"""

house_df.info()

#Drop unnecessary columns
house_df = house_df.drop(['Suburb', 'Address', 'Rooms', 'Landsize', 'BuildingArea', 'Method', 'SellerG', 'Car', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname', 'Propertycount'], axis=1)

house_df

#Convert Date (house sold) column to YearSold
house_df['YearSold'] = pd.to_datetime(house_df['Date'], format='%d/%m/%Y').dt.year

# Remove the original 'Date' column
house_df = house_df.drop('Date', axis=1)

# Rename column
house_df = house_df.rename(columns={'Distance': 'KMfromCBD'})
house_df = house_df.rename(columns={'Bedroom2': 'Bedroom'})

# One-hot-encode "Type" column
house_df['House'] = (house_df['Type'] == 'h').astype(int)
house_df['Unit'] = (house_df['Type'] == 'u').astype(int)
house_df['Townhouse'] = (house_df['Type'] == 't').astype(int)

# Remove the original 'Type' column
house_df = house_df.drop('Type', axis=1)

house_df

# Handle missing values in relevant columns (filling with median or 0 as appropriate)
for column in ['Price', 'Bedroom', 'Bathroom', 'YearBuilt']:
    house_df[column].fillna(house_df[column].median(), inplace=True)

# Drop rows with any NaN values
house_df.dropna(inplace=True)

# Reset the index after cleaning
house_df = house_df.reset_index(drop=True)

# Check for any remaining null/nan values
print(house_df.isnull().sum())
print(house_df.isna().sum())

# Feature engineer
house_df['TotalRooms'] = house_df['Bedroom'] + house_df['Bathroom']
house_df['PricePerRoom'] = round(house_df['Price'] / house_df['TotalRooms'], 0)

house_df.info()

house_df

"""## **Merging the two Dataframes**"""

#Load the dataframes

house_price = merged_df.copy()
house_features = house_df.copy()

house_price

house_features = house_features[house_features['YearBuilt'] >= 1600]
house_features

# Merge the two DataFrames based on the 'Postcode' column
final_df = pd.merge(house_features, house_price, on='Postcode', how='left')

# Display the merged DataFrame
final_df

# Drop rows with any NaN values
final_df.dropna(inplace=True)

# Reset the index after dropping rows
final_df = final_df.reset_index(drop=True)

final_df.info()

"""# **Data Visualization**"""

data = final_df.copy()

"""## Data Analyze"""

data

# Pairplot to visualize relationships between multiple variables
sns.pairplot(data[['Price', 'TotalRooms', 'YearBuilt', 'KMfromCBD']])
plt.show()

# Correlation Matrix Heatmap
correlation_matrix = data.corr()
plt.figure(figsize=(18, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize = (15,8))
sns.countplot(x="TotalRooms", data=data)

plt.figure(figsize=(15, 8))
sns.histplot(data['YearBuilt'], bins=30, kde=True)
plt.title('Distribution of Year Built')
plt.xlabel('Year Built')
plt.ylabel('Frequency')
plt.show()

# Create a figure with two subplots side by side
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Correlation between Price and KMtfromCBD
sns.scatterplot(x='KMfromCBD', y='Price', data=data, ax=axes[0])
axes[0].set_title('Correlation between Price and Distance to CBD')
axes[0].set_xlabel('Distance to CBD (KM)')
axes[0].set_ylabel('Price')

# Correlation between Price and YearBuilt
sns.scatterplot(x='YearBuilt', y='Price', data=data, ax=axes[1])
axes[1].set_title('Correlation between Price and Year Built')
axes[1].set_xlabel('Year Built')
axes[1].set_ylabel('Price')

plt.tight_layout()
plt.show()

# Select the columns representing year-to-year percentage changes
year_to_year_columns = [col for col in data.columns if 'to' in col]

# Calculate the average percentage change for each year
average_changes = data[year_to_year_columns].mean()

# Create a line plot of the average changes
plt.figure(figsize=(10, 6))
plt.plot(average_changes.index, average_changes.values)
plt.xlabel('Year-to-Year Period')
plt.ylabel('Average Percentage Change in House Prices')
plt.title('Average Year-to-Year Change in House Prices')
plt.grid(True)
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

"""## Data Normalization"""

# Create a figure with two subplots side by side
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Histogram of Price
sns.histplot(data['Price'], kde=True, ax=axes[0])
axes[0].set_title('Price Distribution')

# Probability plot for the log-transformed Price
stats.probplot(data['Price'], dist="norm", plot=axes[1])
axes[1].set_title('Probability Plot of Price')

plt.tight_layout()
plt.show()

# Log transform the Price column
data['Price_log'] = np.log(data['Price'] + 1)

# Create a figure with two subplots side by side
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Histogram of Price
sns.histplot(data['Price_log'], kde=True, ax=axes[0])
axes[0].set_title('Price Log Distribution')

# Probability plot for the log-transformed Price
stats.probplot(data['Price_log'], dist="norm", plot=axes[1])
axes[1].set_title('Probability Plot of Log-Transformed Price')

plt.tight_layout()
plt.show()

# Histogram of Price
sns.histplot(house_df['PricePerRoom'], kde=True)
plt.title('PricePerRoom Distribution')
plt.show()

# Probability plot for the log-transformed Price
stats.probplot(house_df['PricePerRoom'], dist="norm", plot=plt)
plt.title('Probability Plot of PricePerRoom')
plt.show()

# Log transform the Price column
data['PricePerRoom_log'] = np.log(data['PricePerRoom'] + 1)

# Visualize the log-transformed data
sns.histplot(data['PricePerRoom_log'], kde=True)
plt.title('Log-Transformed PricePerRoom Distribution')
plt.show()

# Probability plot for the log-transformed Price
stats.probplot(data['PricePerRoom_log'], dist="norm", plot=plt)
plt.title('Probability Plot of Log-Transformed PricePerRoom')
plt.show()

# Correlation Matrix Heatmap
correlation_matrix = data.corr()
plt.figure(figsize=(18, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

"""## Data Outlier"""

# Value < Q1 - 1,5*IQR OR Value > Q3 + 1,5 * IQR
def finding_outliers(data, variable_name) :
    iqr = data[variable_name].quantile(0.75) - data[variable_name].quantile(0.25)
    lower =  data[variable_name].quantile(0.25) -1.5*iqr
    upper =  data[variable_name].quantile(0.75) + 1.5*iqr
    return data [(data[variable_name] < lower) | (data[variable_name] > upper)]

# Price boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="Price", data=data)

# # Price outliers
finding_outliers(data, "Price").sort_values("Price")

# For price
iqr_price = data["Price"].quantile(0.75) - data["Price"].quantile(0.25)
data["Price"].quantile(0.75) + 1.5 * iqr_price
data.loc[(finding_outliers(data, "Price").index, "Price")] = data["Price"].quantile(0.75) + 1.5 * iqr_price

# Price boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="Price", data=data)

# Price boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="PricePerRoom", data=data)

# PricePerRoom outliers
finding_outliers(data, "PricePerRoom").sort_values("PricePerRoom")

# For PricePerRoom
iqr_price = data["PricePerRoom"].quantile(0.75) - data["PricePerRoom"].quantile(0.25)
data["PricePerRoom"].quantile(0.75) + 1.5 * iqr_price
data.loc[(finding_outliers(data, "PricePerRoom").index, "PricePerRoom")] = data["PricePerRoom"].quantile(0.75) + 1.5 * iqr_price

# PricePerRoom boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="PricePerRoom", data=data)

# rooms boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="TotalRooms", data=data)

# TotalRooms outliers
finding_outliers(data, "TotalRooms").sort_values("TotalRooms")

# For TotalRooms
iqr_price = data["TotalRooms"].quantile(0.75) - data["TotalRooms"].quantile(0.25)
data["TotalRooms"].quantile(0.75) + 1.5 * iqr_price
data.loc[(finding_outliers(data, "TotalRooms").index, "TotalRooms")] = data["TotalRooms"].quantile(0.75) + 1.5 * iqr_price

# room boxplot
plt.figure(figsize=(8,8))
sns.boxplot(y="TotalRooms", data=data)

# Plot Bathroom vs Price
plt.figure(figsize=(15,8))
sns.boxplot(x="TotalRooms", y="Price", data=data)

"""# **Machine Learning**"""

df = data.copy()

"""## Data Preparation"""

def classify_price_change(row, column):
    if row[column] < 0:
        return 'Price will drop'
    elif row[column] >= 20:
      return 'Price will go up significantly ( 20% or more )'
    elif row[column] >= 5:
        return 'Price goes up slightly ( 5% or more )'
    else:
        return 'No significant change'

# Apply the function to the desired year (for example, 2022 to 2023)
df['price_class_22_23'] = df.apply(classify_price_change, axis=1, column='2022to2023')

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply the label encoder to the 'price_class' column
df['price_class_encoded'] = label_encoder.fit_transform(df['price_class_22_23'])

"""### Regression preperation"""

X_reg = df.drop(columns=['Price', 'Price_log', 'PricePerRoom_log', 'PricePerRoom','2016to2017','2017to2018','2018to2019', '2019to2020','2020to2021','2021to2022','2022to2023', 'price_class_encoded', 'price_class_22_23'])
y_reg = df['Price_log']  # Using price as the target

# Split the data into training and testing sets
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

"""### Classification preperation"""

# Features for the model (exclude the original 'price_class' column and use the encoded version)
X_clf = df.drop(['price_class_22_23','price_class_encoded','Price','PricePerRoom','2016to2017','2017to2018','2018to2019','2020to2021','2021to2022','2022to2023','Price_log','PricePerRoom_log'], axis=1)  # Your features
y_clf = df['price_class_encoded']  # Encoded target

# Split the data into training and testing sets
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

df

"""### Time-series-forecast preperation (EXPERIMENT)"""

# Calculate the base price for the year 2016
base_price = df[df['YearSold'] == 2016]['Price'].mean()  # Average price for properties sold in 2016
print(f'Base price for 2016: {base_price:.2f}')

# DataFrame containing percentage increases/decreases
data = {
    'Year': ['2016to2017', '2017to2018', '2018to2019', '2019to2020', '2020to2021', '2021to2022', '2022to2023'],
    'PriceChange': [14.106290, -1.516912, -3.462350, 6.859530, 16.616946, -0.159209, -1.879639]
}
df_changes = pd.DataFrame(data)

# Calculate future prices based on the price changes
price_series = [base_price]  # Starting with the base price from 2016
for change in df_changes['PriceChange']:
    new_price = price_series[-1] * (1 + change / 100)  # Calculate new price based on percentage change
    price_series.append(new_price)  # Append new price to the series

# Create a DataFrame with years and calculated prices
years = list(range(2016, 2024))  # Include years 2016 to 2023
# The years list was incorrectly created with years from 2016 to 2030.
# This has been corrected to be from 2016 to 2023 to match the number of items in price_series
price_df = pd.DataFrame({'Year': years, 'AvgPrice': price_series})
price_df.set_index('Year', inplace=True)

"""## Regression Model

### Linear Regression
"""

X_train_reg

# Initialize the model
linear_reg = LinearRegression()

# Train the model
linear_reg.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_linear = linear_reg.predict(X_test_reg)

# Evaluate the model
mae_linear = mean_absolute_error(y_test_reg, y_pred_linear)
mse_linear = mean_squared_error(y_test_reg, y_pred_linear)
r2_linear = r2_score(y_test_reg, y_pred_linear)

print('Linear Regression Performance:')
print(f'MAE: {mae_linear:.4f}')
print(f'MSE: {mse_linear:.4f}')
print(f'R² Score: {r2_linear:.4f}')

"""### Random Forest Regressor"""

# Initialize the model
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_reg.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_rf = rf_reg.predict(X_test_reg)

# Evaluate the model
mae_rf = mean_absolute_error(y_test_reg, y_pred_rf)
mse_rf = mean_squared_error(y_test_reg, y_pred_rf)
r2_rf = r2_score(y_test_reg, y_pred_rf)

print('\nRandom Forest Regressor Performance:')
print(f'MAE: {mae_rf:.4f}')
print(f'MSE: {mse_rf:.4f}')
print(f'R² Score: {r2_rf:.4f}')

"""### Gradient Boosting Regressor"""

# Initialize the model
gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train the model
gb_reg.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_gb = gb_reg.predict(X_test_reg)

# Evaluate the model
mae_gb = mean_absolute_error(y_test_reg, y_pred_gb)
mse_gb = mean_squared_error(y_test_reg, y_pred_gb)
r2_gb = r2_score(y_test_reg, y_pred_gb)

print('\nGradient Boosting Regressor Performance:')
print(f'MAE: {mae_gb:.4f}')
print(f'MSE: {mse_gb:.4f}')
print(f'R² Score: {r2_gb:.4f}')

"""### Decision Tree Regressor"""

# Initialize the model
dt_reg = DecisionTreeRegressor(random_state=42)

# Train the model
dt_reg.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_dt = dt_reg.predict(X_test_reg)

# Evaluate the model
mae_dt = mean_absolute_error(y_test_reg, y_pred_dt)
mse_dt = mean_squared_error(y_test_reg, y_pred_dt)
r2_dt = r2_score(y_test_reg, y_pred_dt)

print('\nDecision Tree Regressor Performance:')
print(f'MAE: {mae_dt:.4f}')
print(f'MSE: {mse_dt:.4f}')
print(f'R² Score: {r2_dt:.4f}')

"""## Visualize Regression ML"""

def plot_predicted_vs_actual(y_test, y_pred, model_name):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.3)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('Actual Log Price')
    plt.ylabel('Predicted Log Price')
    plt.title(f'{model_name}: Predicted vs. Actual Log Prices')
    plt.show()

plot_predicted_vs_actual(y_test_reg, y_pred_linear, 'Linear Regression')

plot_predicted_vs_actual(y_test_reg, y_pred_rf, 'Random Forest Regressor')

plot_predicted_vs_actual(y_test_reg, y_pred_gb, 'Gradient Boosting Regressor')

plot_predicted_vs_actual(y_test_reg, y_pred_dt, 'Decision Tree Regressor')

"""## Classification Model

### Logistic Regression
"""

# Initialize the model
log_reg = LogisticRegression(max_iter=1000, random_state=42)

# Train the model
log_reg.fit(X_train_clf, y_train_clf)

# Make predictions
y_pred_log = log_reg.predict(X_test_clf)

# Evaluate the model
acc_log = accuracy_score(y_test_clf, y_pred_log)

# Use 'weighted' average for multiclass precision
prec_log = precision_score(y_test_clf, y_pred_log, average='weighted')
rec_log = recall_score(y_test_clf, y_pred_log, average='weighted')
f1_log = f1_score(y_test_clf, y_pred_log, average='weighted')

print('Logistic Regression Performance:')
print(f'Accuracy: {acc_log:.4f}')
print(f'Precision: {prec_log:.4f}')
print(f'Recall: {rec_log:.4f}')
print(f'F1 Score: {f1_log:.4f}')

"""### Random Forest Classifier"""

# Initialize the model
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_clf.fit(X_train_clf, y_train_clf)

# Make predictions
y_pred_rf_clf = rf_clf.predict(X_test_clf)

# Evaluate the model
acc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)
# Use 'weighted' average for multiclass precision
prec_rf_clf = precision_score(y_test_clf, y_pred_log, average='weighted')
rec_rf_clf = recall_score(y_test_clf, y_pred_rf_clf, average ='weighted')
f1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average ='weighted')

print('\nRandom Forest Classifier Performance:')
print(f'Accuracy: {acc_rf_clf:.4f}')
print(f'Precision: {prec_rf_clf:.4f}')
print(f'Recall: {rec_rf_clf:.4f}')
print(f'F1 Score: {f1_rf_clf:.4f}')

"""## Visualize Classification ML"""

ConfusionMatrixDisplay.from_estimator(rf_clf, X_test_clf, y_test_clf)
plt.title('Random Forest Classifier Confusion Matrix')
plt.show()

"""## Time Series Forecasting Model (EXPIREMENT)"""

# Model Fitting
model = sm.tsa.ARIMA(price_df['AvgPrice'], order=(1, 1, 1))  # Adjust order based on your data
results = model.fit()

# Print model summary
print(results.summary())

# Forecast the next 7 years (2024 to 2030)
forecast = results.get_forecast(steps=7)
forecast_index = range(price_df.index[-1] + 1, price_df.index[-1] + 8)
forecast_df = forecast.summary_frame()
forecast_df['Year'] = forecast_index
forecast_df.set_index('Year', inplace=True)

# Combine actual and forecasted data
combined_df = pd.concat([price_df, forecast_df[['mean']]], axis=0)

# Plot the forecast
plt.figure(figsize=(10, 6))
plt.plot(price_df.index, price_df['AvgPrice'], label='Observed', marker='o')
plt.plot(forecast_index, forecast_df['mean'], label='Forecast', marker='o', linestyle='--')
plt.fill_between(forecast_index, forecast_df['mean_ci_lower'], forecast_df['mean_ci_upper'], color='pink', alpha=0.3)
plt.title('Average House Price Forecast (2024 - 2030)')
plt.xlabel('Year')
plt.ylabel('Average Price')
plt.legend()
plt.grid()
plt.show()